{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f3f7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec68a6",
   "metadata": {},
   "source": [
    "##### Task 1: Data Preparation on the given corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255f4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The sun sets over the horizon, painting the sky with hues of orange and pink.\",\n",
    "    \"In the heart of the forest, a gentle breeze rustles the leaves.\",\n",
    "    \"She walked along the sandy shore, feeling the cool water on her feet.\",\n",
    "    \"The old bookshop on the corner is filled with stories waiting to be discovered.\",\n",
    "    \"As the rain falls outside, I sit by the window with a cup of hot tea.\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [sentence.split() for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2939d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'sun', 'sets', 'over', 'the', 'horizon,', 'painting', 'the', 'sky', 'with', 'hues', 'of', 'orange', 'and', 'pink.'], ['In', 'the', 'heart', 'of', 'the', 'forest,', 'a', 'gentle', 'breeze', 'rustles', 'the', 'leaves.'], ['She', 'walked', 'along', 'the', 'sandy', 'shore,', 'feeling', 'the', 'cool', 'water', 'on', 'her', 'feet.'], ['The', 'old', 'bookshop', 'on', 'the', 'corner', 'is', 'filled', 'with', 'stories', 'waiting', 'to', 'be', 'discovered.'], ['As', 'the', 'rain', 'falls', 'outside,', 'I', 'sit', 'by', 'the', 'window', 'with', 'a', 'cup', 'of', 'hot', 'tea.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbbd4dd",
   "metadata": {},
   "source": [
    "##### Task 2: Build the N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "758fbbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(words, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = tuple(words[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44e51adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_model(tokenized_corpus, n):\n",
    "    ngram_model = defaultdict(dict)\n",
    "    \n",
    "    for sentence in tokenized_corpus:\n",
    "        ngrams = generate_ngrams(sentence, n)\n",
    "        for i in range(len(ngrams) - 1):\n",
    "            prefix = ngrams[i][:-1]\n",
    "            next_word = ngrams[i][-1]\n",
    "            if next_word in ngram_model[prefix]:\n",
    "                ngram_model[prefix][next_word] += 1\n",
    "            else:\n",
    "                ngram_model[prefix][next_word] = 1\n",
    "                \n",
    "    return ngram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c20a9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_model = build_ngram_model(tokenized_corpus, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72377287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {('The',): {'sun': 1, 'old': 1},\n",
       "             ('sun',): {'sets': 1},\n",
       "             ('sets',): {'over': 1},\n",
       "             ('over',): {'the': 1},\n",
       "             ('the',): {'horizon,': 1,\n",
       "              'sky': 1,\n",
       "              'heart': 1,\n",
       "              'forest,': 1,\n",
       "              'sandy': 1,\n",
       "              'cool': 1,\n",
       "              'corner': 1,\n",
       "              'rain': 1,\n",
       "              'window': 1},\n",
       "             ('horizon,',): {'painting': 1},\n",
       "             ('painting',): {'the': 1},\n",
       "             ('sky',): {'with': 1},\n",
       "             ('with',): {'hues': 1, 'stories': 1, 'a': 1},\n",
       "             ('hues',): {'of': 1},\n",
       "             ('of',): {'orange': 1, 'the': 1, 'hot': 1},\n",
       "             ('orange',): {'and': 1},\n",
       "             ('In',): {'the': 1},\n",
       "             ('heart',): {'of': 1},\n",
       "             ('forest,',): {'a': 1},\n",
       "             ('a',): {'gentle': 1, 'cup': 1},\n",
       "             ('gentle',): {'breeze': 1},\n",
       "             ('breeze',): {'rustles': 1},\n",
       "             ('rustles',): {'the': 1},\n",
       "             ('She',): {'walked': 1},\n",
       "             ('walked',): {'along': 1},\n",
       "             ('along',): {'the': 1},\n",
       "             ('sandy',): {'shore,': 1},\n",
       "             ('shore,',): {'feeling': 1},\n",
       "             ('feeling',): {'the': 1},\n",
       "             ('cool',): {'water': 1},\n",
       "             ('water',): {'on': 1},\n",
       "             ('on',): {'her': 1, 'the': 1},\n",
       "             ('old',): {'bookshop': 1},\n",
       "             ('bookshop',): {'on': 1},\n",
       "             ('corner',): {'is': 1},\n",
       "             ('is',): {'filled': 1},\n",
       "             ('filled',): {'with': 1},\n",
       "             ('stories',): {'waiting': 1},\n",
       "             ('waiting',): {'to': 1},\n",
       "             ('to',): {'be': 1},\n",
       "             ('As',): {'the': 1},\n",
       "             ('rain',): {'falls': 1},\n",
       "             ('falls',): {'outside,': 1},\n",
       "             ('outside,',): {'I': 1},\n",
       "             ('I',): {'sit': 1},\n",
       "             ('sit',): {'by': 1},\n",
       "             ('by',): {'the': 1},\n",
       "             ('window',): {'with': 1},\n",
       "             ('cup',): {'of': 1}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7d9c3",
   "metadata": {},
   "source": [
    "##### Task 3: Generate Text with Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4612201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word_with_smoothing(ngram_model, prefix):\n",
    "    if prefix in ngram_model:\n",
    "        next_words = ngram_model[prefix]\n",
    "        total_count = sum(next_words.values())\n",
    "        probabilities = {word: (count + 1) / (total_count + len(next_words)) for word, count in next_words.items()}\n",
    "        max_prob_word = max(probabilities, key=probabilities.get)\n",
    "        return max_prob_word\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ff0c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_unseen_data(ngram_model, n, test_prefix):\n",
    "    next_word = predict_next_word_with_smoothing(ngram_model, tuple(test_prefix[-n+1:]))\n",
    "    return next_word if next_word is not None else \"No prediction available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beae6aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input_1 = input(\"Enter a sentence for test prefix 1: \")\n",
    "user_input_2 = input(\"Enter a sentence for test prefix 2: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba1e5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input_1 = user_input_1.split()\n",
    "tokenized_input_2 = user_input_2.split()\n",
    "generated_text_user_input_1 = generate_text_with_unseen_data(ngram_model, n=2, test_prefix=tokenized_input_1)\n",
    "generated_text_user_input_2 = generate_text_with_unseen_data(ngram_model, n=2, test_prefix=tokenized_input_2)\n",
    "generated_sentence_1 = \" \".join(tokenized_input_1 + [generated_text_user_input_1])\n",
    "generated_sentence_2 = \" \".join(tokenized_input_2 + [generated_text_user_input_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e07bb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text for User Input 1: The sun sets\n",
      "Generated Text for User Input 2: She walked\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Text for User Input 1:\", generated_sentence_1)\n",
    "print(\"Generated Text for User Input 2:\", generated_sentence_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
